{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import tensorflow_hub as hub\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # convert to lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # remove stopwords and lemmatize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# preprocess the data\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "X = np.array([embed([preprocess_text(q + ' ' + a)]).numpy().flatten() for q, a in zip(data['question'], data['student_answer'])])\n",
        "y = data['score_avg']\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# create the model and fit it to the training data\n",
        "model = MLPRegressor(hidden_layer_sizes=(100,50), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate the model on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# compute evaluation metrics\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# print evaluation metrics\n",
        "print('R-squared:', r2)\n",
        "print('Mean absolute error:', mae)\n",
        "print('Mean squared error:', mse)\n",
        "print('Root mean squared error:', rmse)\n",
        "\n",
        "# deploy the model to production\n",
        "def predict_relevance(question, desired_answer, student_answer):\n",
        "    # preprocess the text\n",
        "    question = preprocess_text(question)\n",
        "    desired_answer = preprocess_text(desired_answer)\n",
        "    student_answer = preprocess_text(student_answer)\n",
        "\n",
        "    # concatenate question and student_answer texts\n",
        "    input_text = question + ' ' + student_answer\n",
        "    \n",
        "    # encode input text using Universal Sentence Encoder\n",
        "    input_emb = embed([input_text]).numpy().flatten()\n",
        "    \n",
        "    # predict the relevance score of the input text\n",
        "    score = model.predict([input_emb])[0]\n",
        "    print(score)\n",
        "    # compute cosine similarity between desired_answer and student_answer texts\n",
        "    similarity = cosine_similarity(embed([desired_answer]).numpy(), embed([student_answer]).numpy())[0][0]\n",
        "    print(\"Cosine \", similarity)\n",
        "    \n",
        "    # adjust score based on similarity\n",
        "    if similarity < 0.4:\n",
        "        score -= 1\n",
        "    elif similarity >= 0.4 and similarity < 0.8:\n",
        "        score += 1\n",
        "    else:\n",
        "        score += 2\n",
        "    \n",
        "    # ensure score is between 0 and 5\n",
        "    score = min(max(score, 0), 5)\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "question = \"What is the difference between an array declared as static, and one that is not?\"\n",
        "desired_answer = \"The arrays declared as static live throughout the life of the program; that is, they are initialized only once, when the function that declares the array it is first called.\"\n",
        "student_answer = \"a static array will store the new values that were assigned to each of its elements. meaning if you call a function twice it will use the last values that were returned the first time.  if you don't declare it static then the new values will not be stored and will be reset to their original value\"\n",
        "new_answer = \"adt adt adt adt adt adt\"\n",
        "newAnswer = \"An array declared as static has a fixed size and retains its value between function calls, while a non-static array does not retain its value and its size can change during runtime.\"\n",
        "print(\"=================== Irrelevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, new_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"I don't know\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static one stays while non static one leaves\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"one is memory allocated while other is storage allocated\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is like a classic car that never goes out of style, while the non-static array is like a trendy new smartphone that quickly becomes outdated.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static array, non-static array, static array non-static array static array non-static array static array non-static array static array non-static array static array non-static array \"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"An array is something which starts with square brackets and all values inside it are seaperated by commas thats why it is static\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is something that is not non static. Thats why it is called as static array.\"))\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"=================== relevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, student_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, newAnswer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array persists throughout the lifetime of a program, while a non-static array is deallocated once it goes out of scope.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array is initialized to zero by default, while a non-static array has an undefined initial value unless explicitly initialized.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be accessed from any function within a file, while a non-static array is only accessible within the function in which it is declared.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be initialized only once, at compile time, while a non-static array can be initialized multiple times, including at runtime.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can have a fixed size that is determined at compile time, while a non-static array can have a variable size that is determined at runtime.\"))\n",
        "print()\n"
      ],
      "metadata": {
        "id": "faJJRAEwcPDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import tensorflow_hub as hub\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # convert to lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # remove stopwords and lemmatize\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# preprocess the data\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "X = np.array([embed([preprocess_text(q + ' ' + a)]).numpy().flatten() for q, a in zip(data['question'], data['student_answer'])])\n",
        "y = data['score_avg']\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# create the model and fit it to the training data\n",
        "model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate the model on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# compute evaluation metrics\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# print evaluation metrics\n",
        "print('R-squared:', r2)\n",
        "print('Mean absolute error:', mae)\n",
        "print('Mean squared error:', mse)\n",
        "print('Root mean squared error:', rmse)\n",
        "\n",
        "# deploy the model to production\n",
        "def predict_relevance(question, desired_answer, student_answer):\n",
        "    # preprocess the text\n",
        "    question = preprocess_text(question)\n",
        "    desired_answer = preprocess_text(desired_answer)\n",
        "    student_answer = preprocess_text(student_answer)\n",
        "\n",
        "    # concatenate question and student_answer texts\n",
        "    input_text = question + ' ' + student_answer\n",
        "    \n",
        "    # encode input text using Universal Sentence Encoder\n",
        "    input_emb = embed([input_text]).numpy().flatten()\n",
        "    \n",
        "    # predict the relevance score of the input text\n",
        "    score = model.predict([input_emb])[0]\n",
        "    print(score)\n",
        "    # compute cosine similarity between desired_answer and student_answer texts\n",
        "    similarity = cosine_similarity(embed([desired_answer]).numpy(), embed([student_answer]).numpy())[0][0]\n",
        "    print(\"Cosine \", similarity)\n",
        "    \n",
        "    # adjust score based on similarity\n",
        "    if similarity < 0.4:\n",
        "        score -= 1\n",
        "    elif similarity >= 0.4 and similarity < 0.8:\n",
        "        score += 1\n",
        "    else:\n",
        "        score += 2\n",
        "    \n",
        "    # ensure score is between 0 and 5\n",
        "    score = min(max(score, 0), 5)\n",
        "    \n",
        "    return score\n",
        "\n",
        "\n",
        "question = \"What is the difference between an array declared as static, and one that is not?\"\n",
        "desired_answer = \"The arrays declared as static live throughout the life of the program; that is, they are initialized only once, when the function that declares the array it is first called.\"\n",
        "student_answer = \"a static array will store the new values that were assigned to each of its elements. meaning if you call a function twice it will use the last values that were returned the first time.  if you don't declare it static then the new values will not be stored and will be reset to their original value\"\n",
        "new_answer = \"adt adt adt adt adt adt\"\n",
        "newAnswer = \"An array declared as static has a fixed size and retains its value between function calls, while a non-static array does not retain its value and its size can change during runtime.\"\n",
        "print(\"=================== Irrelevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, new_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"I don't know\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static one stays while non static one leaves\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"one is memory allocated while other is storage allocated\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is like a classic car that never goes out of style, while the non-static array is like a trendy new smartphone that quickly becomes outdated.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static array, non-static array, static array non-static array static array non-static array static array non-static array static array non-static array static array non-static array \"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"An array is something which starts with square brackets and all values inside it are seaperated by commas thats why it is static\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is something that is not non static. Thats why it is called as static array.\"))\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"=================== relevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, student_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, newAnswer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array persists throughout the lifetime of a program, while a non-static array is deallocated once it goes out of scope.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array is initialized to zero by default, while a non-static array has an undefined initial value unless explicitly initialized.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be accessed from any function within a file, while a non-static array is only accessible within the function in which it is declared.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be initialized only once, at compile time, while a non-static array can be initialized multiple times, including at runtime.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can have a fixed size that is determined at compile time, while a non-static array can have a variable size that is determined at runtime.\"))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dnBDwKdjyyr",
        "outputId": "c344aeac-73f5-4ed1-d621-4ae5f27163b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared: 0.3568007295451401\n",
            "Mean absolute error: 0.6416389395914556\n",
            "Mean squared error: 0.8269381597435779\n",
            "Root mean squared error: 0.9093614021628463\n",
            "=================== Irrelevant answers ==========================\n",
            "2.897167617455096\n",
            "Cosine  0.14655071\n",
            "1.897167617455096\n",
            "\n",
            "1.453086525489041\n",
            "Cosine  -0.016367897\n",
            "0.45308652548904105\n",
            "\n",
            "2.870634974206055\n",
            "Cosine  0.2534617\n",
            "1.8706349742060548\n",
            "\n",
            "2.780173678593783\n",
            "Cosine  0.32066128\n",
            "1.780173678593783\n",
            "\n",
            "3.68748518799407\n",
            "Cosine  0.3012029\n",
            "2.68748518799407\n",
            "\n",
            "3.2204192687397604\n",
            "Cosine  0.56606376\n",
            "4.220419268739761\n",
            "\n",
            "2.4229418097225444\n",
            "Cosine  0.506612\n",
            "3.4229418097225444\n",
            "\n",
            "3.9501894883596687\n",
            "Cosine  0.56753397\n",
            "4.950189488359669\n",
            "\n",
            "=================== relevant answers ==========================\n",
            "4.9001978594859\n",
            "Cosine  0.46199876\n",
            "5\n",
            "\n",
            "4.320114097287814\n",
            "Cosine  0.5833217\n",
            "5\n",
            "\n",
            "4.378520515014515\n",
            "Cosine  0.70521516\n",
            "5\n",
            "\n",
            "4.448256202369251\n",
            "Cosine  0.6507589\n",
            "5\n",
            "\n",
            "3.875730990502816\n",
            "Cosine  0.60621214\n",
            "4.8757309905028166\n",
            "\n",
            "4.129722257868863\n",
            "Cosine  0.6724781\n",
            "5\n",
            "\n",
            "4.871960606126379\n",
            "Cosine  0.52879477\n",
            "5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyiho1VB3JdT",
        "outputId": "b1b65b27-f4b4-40a3-f21e-3b9d6150b3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # convert to lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # remove stopwords\n",
        "    tokens = text.split()\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "# load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# preprocess the data\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "X = np.array([preprocess_text(q + ' ' + a) for q, a in zip(data['question'], data['student_answer'])])\n",
        "y = data['score_avg']\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# define the LSTM model\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False),\n",
        "    tf.keras.layers.Reshape((512,), input_shape=(1, 512)),\n",
        "    tf.keras.layers.RepeatVector(32),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation='relu', return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, activation='relu')),\n",
        "    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "\n",
        "# define early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# fit the model to the training data with early stopping\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=16, callbacks=[early_stop])\n",
        "\n",
        "# evaluate the model on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# compute evaluation metrics\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# print evaluation metrics\n",
        "print('R-squared:', r2)\n",
        "print('Mean absolute error:', mae)\n",
        "print('Mean squared error:', mse)\n",
        "print('Root mean squared error:', rmse)\n",
        "\n",
        "# deploy the model to production\n",
        "def predict_relevance(question, desired_answer, student_answer):\n",
        "    # preprocess the text\n",
        "    question = preprocess_text(question)\n",
        "    desired_answer = preprocess_text(desired_answer)\n",
        "    student_answer = preprocess_text(student_answer)\n",
        "\n",
        "    # concatenate question and student_answer texts\n",
        "    input_text = question + ' ' + student_answer\n",
        "    \n",
        "    # predict the relevance score of the input text\n",
        "    score = model.predict([input_text])[0][0]\n",
        "    \n",
        "    print(score)\n",
        "    # compute cosine similarity between desired_answer and student_answer texts\n",
        "    similarity = cosine_similarity(embed([desired_answer]).numpy(), embed([student_answer]).numpy())[0][0]\n",
        "    print(\"Cosine \", similarity)\n",
        "    \n",
        "    # adjust score based on similarity\n",
        "    if similarity < 0.4:\n",
        "        score -= 1\n",
        "    elif similarity >= 0.4 and similarity < 0.8:\n",
        "        score += 1\n",
        "    else:\n",
        "        score += 2\n",
        "    \n",
        "    # ensure score is between 0 and 5\n",
        "    score = min(max(score, 0), 5)\n",
        "    \n",
        "    return score\n",
        "\n",
        "question = \"What is the difference between an array declared as static, and one that is not?\"\n",
        "desired_answer = \"The arrays declared as static live throughout the life of the program; that is, they are initialized only once, when the function that declares the array it is first called.\"\n",
        "student_answer = \"a static array will store the new values that were assigned to each of its elements. meaning if you call a function twice it will use the last values that were returned the first time.  if you don't declare it static then the new values will not be stored and will be reset to their original value\"\n",
        "new_answer = \"adt adt adt adt adt adt\"\n",
        "newAnswer = \"An array declared as static has a fixed size and retains its value between function calls, while a non-static array does not retain its value and its size can change during runtime.\"\n",
        "print(\"=================== Irrelevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, new_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"I don't know\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static one stays while non static one leaves\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"one is memory allocated while other is storage allocated\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is like a classic car that never goes out of style, while the non-static array is like a trendy new smartphone that quickly becomes outdated.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static array, non-static array, static array non-static array static array non-static array static array non-static array static array non-static array static array non-static array \"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"An array is something which starts with square brackets and all values inside it are seaperated by commas thats why it is static\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is something that is not non static. Thats why it is called as static array.\"))\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"=================== relevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, student_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, newAnswer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array persists throughout the lifetime of a program, while a non-static array is deallocated once it goes out of scope.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array is initialized to zero by default, while a non-static array has an undefined initial value unless explicitly initialized.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be accessed from any function within a file, while a non-static array is only accessible within the function in which it is declared.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be initialized only once, at compile time, while a non-static array can be initialized multiple times, including at runtime.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can have a fixed size that is determined at compile time, while a non-static array can have a variable size that is determined at runtime.\"))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEv75gMV4hNV",
        "outputId": "46eb78c3-f22a-439b-d216-b44437d7ca81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "91/91 [==============================] - 53s 353ms/step - loss: 2.4640 - mean_absolute_error: 1.1762 - mean_squared_error: 2.4640 - val_loss: 0.8494 - val_mean_absolute_error: 0.7248 - val_mean_squared_error: 0.8494\n",
            "Epoch 2/10\n",
            "91/91 [==============================] - 27s 295ms/step - loss: 0.9428 - mean_absolute_error: 0.7476 - mean_squared_error: 0.9428 - val_loss: 0.7503 - val_mean_absolute_error: 0.6589 - val_mean_squared_error: 0.7503\n",
            "Epoch 3/10\n",
            "91/91 [==============================] - 27s 300ms/step - loss: 0.7731 - mean_absolute_error: 0.6652 - mean_squared_error: 0.7731 - val_loss: 0.7300 - val_mean_absolute_error: 0.6601 - val_mean_squared_error: 0.7300\n",
            "Epoch 4/10\n",
            "91/91 [==============================] - 28s 308ms/step - loss: 0.6528 - mean_absolute_error: 0.6076 - mean_squared_error: 0.6528 - val_loss: 1.2641 - val_mean_absolute_error: 0.9719 - val_mean_squared_error: 1.2641\n",
            "Epoch 5/10\n",
            "91/91 [==============================] - 27s 293ms/step - loss: 0.6238 - mean_absolute_error: 0.6048 - mean_squared_error: 0.6238 - val_loss: 0.6981 - val_mean_absolute_error: 0.6292 - val_mean_squared_error: 0.6981\n",
            "Epoch 6/10\n",
            "91/91 [==============================] - 27s 295ms/step - loss: 0.4621 - mean_absolute_error: 0.4966 - mean_squared_error: 0.4621 - val_loss: 0.6970 - val_mean_absolute_error: 0.5963 - val_mean_squared_error: 0.6970\n",
            "Epoch 7/10\n",
            "91/91 [==============================] - 27s 294ms/step - loss: 0.3625 - mean_absolute_error: 0.4218 - mean_squared_error: 0.3625 - val_loss: 0.6887 - val_mean_absolute_error: 0.6064 - val_mean_squared_error: 0.6887\n",
            "Epoch 8/10\n",
            "91/91 [==============================] - 27s 293ms/step - loss: 0.4607 - mean_absolute_error: 0.4662 - mean_squared_error: 0.4607 - val_loss: 0.7944 - val_mean_absolute_error: 0.6614 - val_mean_squared_error: 0.7944\n",
            "Epoch 9/10\n",
            "91/91 [==============================] - 26s 286ms/step - loss: 0.4077 - mean_absolute_error: 0.4534 - mean_squared_error: 0.4077 - val_loss: 0.9777 - val_mean_absolute_error: 0.8469 - val_mean_squared_error: 0.9777\n",
            "Epoch 10/10\n",
            "91/91 [==============================] - 26s 284ms/step - loss: 0.3412 - mean_absolute_error: 0.4184 - mean_squared_error: 0.3412 - val_loss: 0.6367 - val_mean_absolute_error: 0.5843 - val_mean_squared_error: 0.6367\n",
            "15/15 [==============================] - 1s 30ms/step\n",
            "R-squared: 0.34297537002504863\n",
            "Mean absolute error: 0.6165607263753702\n",
            "Mean squared error: 0.8122217063227222\n",
            "Root mean squared error: 0.9012334360878552\n",
            "=================== Irrelevant answers ==========================\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1.5764737\n",
            "Cosine  0.15621793\n",
            "0.5764737129211426\n",
            "\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1.6514018\n",
            "Cosine  0.0060708523\n",
            "0.6514017581939697\n",
            "\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "2.4003417\n",
            "Cosine  0.2627005\n",
            "1.4003417491912842\n",
            "\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "3.8555293\n",
            "Cosine  0.31882492\n",
            "2.855529308319092\n",
            "\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "2.5849066\n",
            "Cosine  0.30789843\n",
            "1.5849065780639648\n",
            "\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "3.1605184\n",
            "Cosine  0.55918676\n",
            "4.160518407821655\n",
            "\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "2.9564688\n",
            "Cosine  0.45380533\n",
            "3.9564688205718994\n",
            "\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "3.056217\n",
            "Cosine  0.55349475\n",
            "4.0562169551849365\n",
            "\n",
            "=================== relevant answers ==========================\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "3.4422517\n",
            "Cosine  0.477167\n",
            "4.442251682281494\n",
            "\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "3.6194117\n",
            "Cosine  0.5726348\n",
            "4.6194117069244385\n",
            "\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "3.8958628\n",
            "Cosine  0.7027976\n",
            "4.895862817764282\n",
            "\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "4.092353\n",
            "Cosine  0.64588\n",
            "5\n",
            "\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "3.013603\n",
            "Cosine  0.58219373\n",
            "4.01360297203064\n",
            "\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "4.305527\n",
            "Cosine  0.69239616\n",
            "5\n",
            "\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "3.8054116\n",
            "Cosine  0.52618045\n",
            "4.8054115772247314\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # convert to lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # remove stopwords\n",
        "    tokens = text.split()\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "# load the dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# preprocess the data\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "X = np.array([preprocess_text(q + ' ' + a) for q, a in zip(data['question'], data['student_answer'])])\n",
        "y = data['score_avg']\n",
        "\n",
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# define the LSTM model\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", input_shape=[], dtype=tf.string, trainable=False),\n",
        "    tf.keras.layers.Reshape((512,), input_shape=(1, 512)),\n",
        "    tf.keras.layers.RepeatVector(32),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, activation='relu', return_sequences=True, kernel_regularizer=regularizers.l2(0.01))),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))),\n",
        "    tf.keras.layers.Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "\n",
        "# define early stopping\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# fit the model to the training data with early stopping\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=16, callbacks=[early_stop])\n",
        "\n",
        "# evaluate the model on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# compute evaluation metrics\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# print evaluation metrics\n",
        "print('R-squared:', r2)\n",
        "print('Mean absolute error:', mae)\n",
        "print('Mean squared error:', mse)\n",
        "print('Root mean squared error:', rmse)\n",
        "\n",
        "# deploy the model to production\n",
        "def predict_relevance(question, desired_answer, student_answer):\n",
        "    # preprocess the text\n",
        "    question = preprocess_text(question)\n",
        "    desired_answer = preprocess_text(desired_answer)\n",
        "    student_answer = preprocess_text(student_answer)\n",
        "\n",
        "    # concatenate question and student_answer texts\n",
        "    input_text = question + ' ' + student_answer\n",
        "    \n",
        "    # predict the relevance score of the input text\n",
        "    score = model.predict([input_text])[0][0]\n",
        "    print(score)\n",
        "    \n",
        "    # compute cosine similarity between desired_answer and student_answer texts\n",
        "    similarity = cosine_similarity(embed([desired_answer]).numpy(), embed([student_answer]).numpy())[0][0]\n",
        "    print(\"Cosine \", similarity)\n",
        "    \n",
        "    # adjust score based on similarity\n",
        "    if similarity < 0.4:\n",
        "        score -= 1\n",
        "    elif similarity >= 0.4 and similarity < 0.8:\n",
        "        score += 1\n",
        "    else:\n",
        "        score += 2\n",
        "    \n",
        "    # ensure score is between 0 and 5\n",
        "    score = min(max(score, 0), 5)\n",
        "    \n",
        "    return score\n",
        "\n",
        "question = \"What is the difference between an array declared as static, and one that is not?\"\n",
        "desired_answer = \"The arrays declared as static live throughout the life of the program; that is, they are initialized only once, when the function that declares the array it is first called.\"\n",
        "student_answer = \"a static array will store the new values that were assigned to each of its elements. meaning if you call a function twice it will use the last values that were returned the first time.  if you don't declare it static then the new values will not be stored and will be reset to their original value\"\n",
        "new_answer = \"adt adt adt adt adt adt\"\n",
        "newAnswer = \"An array declared as static has a fixed size and retains its vealue between function calls, while a non-static array does not retain its value and its size can change during runtime.\"\n",
        "print(\"=================== Irrelevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, new_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"I don't know\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static one stays while non static one leaves\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"one is memory allocated while other is storage allocated\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is like a classic car that never goes out of style, while the non-static array is like a trendy new smartphone that quickly becomes outdated.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"static array, non-static array, static array non-static array static array non-static array static array non-static array static array non-static array static array non-static array \"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"An array is something which starts with square brackets and all values inside it are seaperated by commas thats why it is static\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"The static array is something that is not non static. Thats why it is called as static array.\"))\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"=================== relevant answers ==========================\")\n",
        "print(predict_relevance(question, desired_answer, student_answer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, newAnswer))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array persists throughout the lifetime of a program, while a non-static array is deallocated once it goes out of scope.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array is initialized to zero by default, while a non-static array has an undefined initial value unless explicitly initialized.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be accessed from any function within a file, while a non-static array is only accessible within the function in which it is declared.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can be initialized only once, at compile time, while a non-static array can be initialized multiple times, including at runtime.\"))\n",
        "print()\n",
        "print(predict_relevance(question, desired_answer, \"A static array can have a fixed size that is determined at compile time, while a non-static array can have a variable size that is determined at runtime.\"))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkixbtscjirQ",
        "outputId": "fb48293c-c08c-4182-ec8d-ac09da9d31a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "91/91 [==============================] - 50s 356ms/step - loss: 71.0695 - mean_absolute_error: 3.1224 - mean_squared_error: 57.9115 - val_loss: 13.1515 - val_mean_absolute_error: 1.3109 - val_mean_squared_error: 2.6152\n",
            "Epoch 2/10\n",
            "91/91 [==============================] - 26s 281ms/step - loss: 11.3099 - mean_absolute_error: 0.8978 - mean_squared_error: 1.3152 - val_loss: 10.7195 - val_mean_absolute_error: 0.9213 - val_mean_squared_error: 1.2258\n",
            "Epoch 3/10\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 10.2171 - mean_absolute_error: 0.8369 - mean_squared_error: 1.0963 - val_loss: 9.9336 - val_mean_absolute_error: 0.8953 - val_mean_squared_error: 1.1598\n",
            "Epoch 4/10\n",
            "91/91 [==============================] - 27s 299ms/step - loss: 9.4970 - mean_absolute_error: 0.7783 - mean_squared_error: 1.0119 - val_loss: 9.3876 - val_mean_absolute_error: 0.7917 - val_mean_squared_error: 1.1813\n",
            "Epoch 5/10\n",
            "91/91 [==============================] - 28s 304ms/step - loss: 8.9519 - mean_absolute_error: 0.7633 - mean_squared_error: 0.9681 - val_loss: 8.8884 - val_mean_absolute_error: 0.7768 - val_mean_squared_error: 1.1283\n",
            "Epoch 6/10\n",
            "91/91 [==============================] - 26s 289ms/step - loss: 8.4514 - mean_absolute_error: 0.7289 - mean_squared_error: 0.9026 - val_loss: 8.4092 - val_mean_absolute_error: 0.7588 - val_mean_squared_error: 1.0646\n",
            "Epoch 7/10\n",
            "91/91 [==============================] - 27s 296ms/step - loss: 7.9837 - mean_absolute_error: 0.6928 - mean_squared_error: 0.8178 - val_loss: 8.0163 - val_mean_absolute_error: 0.8004 - val_mean_squared_error: 1.0313\n",
            "Epoch 8/10\n",
            "91/91 [==============================] - 26s 288ms/step - loss: 7.6084 - mean_absolute_error: 0.6729 - mean_squared_error: 0.7796 - val_loss: 7.7509 - val_mean_absolute_error: 0.7927 - val_mean_squared_error: 1.0781\n",
            "Epoch 9/10\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 7.2440 - mean_absolute_error: 0.6316 - mean_squared_error: 0.7136 - val_loss: 7.3922 - val_mean_absolute_error: 0.7221 - val_mean_squared_error: 0.9990\n",
            "Epoch 10/10\n",
            "91/91 [==============================] - 26s 286ms/step - loss: 6.8860 - mean_absolute_error: 0.5879 - mean_squared_error: 0.6225 - val_loss: 7.1470 - val_mean_absolute_error: 0.8110 - val_mean_squared_error: 1.0112\n",
            "15/15 [==============================] - 1s 30ms/step\n",
            "R-squared: 0.22154515177082867\n",
            "Mean absolute error: 0.7521212661659324\n",
            "Mean squared error: 0.9042577720454509\n",
            "Root mean squared error: 0.9509246931515928\n",
            "=================== Irrelevant answers ==========================\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "2.1331456\n",
            "Cosine  0.15621793\n",
            "1.1331455707550049\n",
            "\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "2.6106246\n",
            "Cosine  0.006070841\n",
            "1.6106245517730713\n",
            "\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "2.759438\n",
            "Cosine  0.2627005\n",
            "1.7594380378723145\n",
            "\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "3.5820034\n",
            "Cosine  0.31882495\n",
            "2.582003355026245\n",
            "\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "2.6903641\n",
            "Cosine  0.30789843\n",
            "1.690364122390747\n",
            "\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "3.4710207\n",
            "Cosine  0.5591867\n",
            "4.471020698547363\n",
            "\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "3.0040102\n",
            "Cosine  0.45380533\n",
            "4.004010200500488\n",
            "\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "3.2498345\n",
            "Cosine  0.55349475\n",
            "4.2498345375061035\n",
            "\n",
            "=================== relevant answers ==========================\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "3.8645263\n",
            "Cosine  0.47716698\n",
            "4.864526271820068\n",
            "\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "3.968431\n",
            "Cosine  0.59303045\n",
            "4.968430995941162\n",
            "\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "3.8104968\n",
            "Cosine  0.70279765\n",
            "4.810496807098389\n",
            "\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "3.9276307\n",
            "Cosine  0.6458799\n",
            "4.927630662918091\n",
            "\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "3.9344177\n",
            "Cosine  0.58219373\n",
            "4.934417724609375\n",
            "\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "3.5272882\n",
            "Cosine  0.6923961\n",
            "4.527288198471069\n",
            "\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "3.9466922\n",
            "Cosine  0.5261805\n",
            "4.946692228317261\n",
            "\n"
          ]
        }
      ]
    }
  ]
}